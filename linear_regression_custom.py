# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBkRx3zcZqLB5_Zuq6uHAsPLXGc655Hl
"""

"""
linear_regression_custom.py

Custom implementation of Linear Regression with L1/L2 regularization,
early stopping, and full-batch gradient descent.
"""

import numpy as np
import matplotlib.pyplot as plt

class LinearRegressionCustom:
    def __init__(self, n_iter=10, learning_rate=1e-2, C=1.0, early_stopping=5,
                 penalty='l2', eps=1e-6, random_state=1):
        """
        Initialize the linear regression model with hyperparameters.
        """
        self.n_iter = n_iter                      # Maximum number of iterations (epochs)
        self.learning_rate = learning_rate        # Gradient descent step size
        self.C = C                                # Inverse regularization strength
        self.early_stopping = early_stopping      # Patience for early stopping
        self.penalty = penalty                    # Regularization type: 'l1' or 'l2'
        self.eps = eps                            # Tolerance for early stopping
        self.random_state = random_state          # Reproducibility seed

    def fit(self, x, y):
        """
        Train the model using full-batch gradient descent.
        """
        batch, input_dim = x.shape

        # Sanity checks
        assert batch == len(y), "Input batch size must match target size"
        assert self.penalty in ('l1', 'l2'), "Penalty must be either 'l1' or 'l2'"

        # Initialize weights with small random values
        rng = np.random.RandomState(seed=self.random_state)
        self.w_ = rng.normal(loc=0.0, scale=0.1, size=input_dim)
        self.b_ = 0.0  # Bias term

        self.losses_ = []            # Store loss over epochs for analysis
        no_improve_count = 0         # Counter for early stopping

        for _ in range(self.n_iter):
            y_pred = self.net_input(x)
            error = y - y_pred       # Residuals

            # Bias update: average gradient across batch
            self.b_ += self.learning_rate * error.mean()

            # Weight update with L1 or L2 penalty
            if self.penalty == 'l2':
                grad = error.dot(x) / batch - 2 * (self.C ** -1) * self.w_
            else:  # L1 regularization
                grad = error.dot(x) / batch - (self.C ** -1) * np.sign(self.w_)

            self.w_ += self.learning_rate * grad

            # Compute and record MSE loss
            loss = (error ** 2).mean()
            self.losses_.append(loss)

            # Early stopping logic: stop if improvement is below eps for N rounds
            if len(self.losses_) > 1 and abs(self.losses_[-1] - self.losses_[-2]) < self.eps:
                no_improve_count += 1
            else:
                no_improve_count = 0

            if no_improve_count > self.early_stopping:
                break

        return self

    def net_input(self, x):
        """
        Compute the linear combination of inputs and weights.
        """
        return x.dot(self.w_) + self.b_

    def predict(self, x):
        """
        Predict target values for new input data.
        """
        return self.net_input(x)

    def loss_fn(self, x, y_true):
        """
        Compute the regularized mean squared error loss.
        """
        y_pred = self.net_input(x)

        # Regularization term
        if self.penalty == 'l2':
            reg = (self.C ** -1) * np.sum(self.w_ ** 2)
        else:
            reg = (self.C ** -1) * np.sum(np.abs(self.w_))

        # Total loss = MSE + regularization
        return np.mean((y_true - y_pred) ** 2) + reg

    def score(self, x, y_true):
        """
        Calculate the R-squared (coefficient of determination).
        """
        y_pred = self.predict(x)
        u = np.sum((y_true - y_pred) ** 2)              # Residual sum of squares
        v = np.sum((y_true - np.mean(y_true)) ** 2)     # Total sum of squares
        return 1 - u / v

    def __repr__(self):
        """
        String representation of the model with current hyperparameters.
        """
        return (f"LinearRegressionCustom(n_iter={self.n_iter}, learning_rate={self.learning_rate}, "
                f"C={self.C}, early_stopping={self.early_stopping}, penalty='{self.penalty}', "
                f"eps={self.eps}, random_state={self.random_state})")

def lin_regplot(X, y, model):
    """
    Visualize the regression line against the data.
    """
    plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)
    plt.plot(X, model.predict(X), color='black', lw=2)
    plt.xlabel("Feature")
    plt.ylabel("Target")
    plt.title("Linear Regression Fit")